{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decf577b-59d4-470d-85df-23e83de2df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce878a8e27194dbfb6b19eb8328e8a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3125), Label(value='0 / 3125'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "from tokenizers import Tokenizer\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "from functools import partial\n",
    "import emoji\n",
    "\n",
    "# Change dataset paths below if needed\n",
    "\n",
    "CONFIG = {\n",
    "    \"max_seq_len\": 768,\n",
    "    \"data_path\": \"Amazon/Movies_and_TV_reviews.parquet\",\n",
    "    \"tokenizer_path\": \"movie_review_tokenizer.json\",\n",
    "    \"padded_token_ids\": \"Amazon/A_padded_token_ids.pt\",\n",
    "    \"padded_attention_masks\": \"Amazon/A_padded_attention_masks.pt\",\n",
    "    \"sentiment_labels\": \"Amazon/A_sentiment_labels.pt\",\n",
    "    \"nmb_workers\": 8\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# 1. Reading and cleaning data\n",
    "# =======================\n",
    "\n",
    "df = pd.read_parquet(CONFIG[\"data_path\"])\n",
    "\n",
    "def clean_text(text, bs4_parser, regex_module, emoji_module):\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove HTML tags\n",
    "    text = bs4_parser(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = regex_module.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=regex_module.MULTILINE)\n",
    "\n",
    "    # 4. Remove emojis\n",
    "    text = emoji_module.replace_emoji(text, replace='')\n",
    "    \n",
    "    # 5. Remove problematic chars (keep emoticons)\n",
    "    text = regex_module.sub(r'[\\x00-\\x1F\\x7F-\\x9F\\u2000-\\u200F\\u2028-\\u202F]', '', text)\n",
    "    text = regex_module.sub(\n",
    "        r'[^\\w\\s.,!?\\':)(/-=;]', \n",
    "        '', \n",
    "        str(text)\n",
    "    )\n",
    "    text = regex_module.sub(r'(?<!\\w)[:=/(](?!\\w)', '', text)\n",
    "    \n",
    "    # 6. Remove extra whitespace\n",
    "    text = regex_module.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create a pre-configured cleaning function and apply in parallel\n",
    "clean_text_optimized = partial(clean_text, bs4_parser=BeautifulSoup, regex_module=re, emoji_module=emoji)\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=CONFIG[\"nmb_workers\"])\n",
    "df[\"cleaned_text\"] = df[\"text\"].parallel_apply(clean_text_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b15356-20e4-4750-ad02-533cf58acc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bff64ea7e7f433baf55f650c7f1cdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3125), Label(value='0 / 3125'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 6181\n",
      "Mean length: 68.78504\n",
      "Median length: 27.0\n",
      "95th percentile: 276.0\n",
      "99th percentile: 640.0099999999984\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# 2. Tokenizing data\n",
    "# =======================\n",
    "\n",
    "def token_get(review, tokenizer):\n",
    "    return tokenizer.encode(review, add_special_tokens=True).ids\n",
    "\n",
    "# Parallel tokenization and length calculation\n",
    "tokenizer = Tokenizer.from_file(CONFIG[\"tokenizer_path\"])\n",
    "token_get_tokenizer = partial(token_get, tokenizer=tokenizer)\n",
    "df[\"tokenized_text\"] = df[\"cleaned_text\"].parallel_apply(token_get_tokenizer)\n",
    "df[\"token_length\"] = df[\"tokenized_text\"].str.len()\n",
    "\n",
    "# Get lengths as list\n",
    "lengths = df[\"token_length\"].tolist()\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"Mean length: {np.mean(lengths)}\")\n",
    "print(f\"Median length: {np.median(lengths)}\")\n",
    "print(f\"95th percentile: {np.percentile(lengths, 95)}\")\n",
    "print(f\"99th percentile: {np.percentile(lengths, 99)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d5e8622-bf3e-4458-b64e-781c221c828c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3a4a1551cd4ef8b450048b721c71d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3125), Label(value='0 / 3125'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared - Samples: 25000, \n",
      "Shapes:\n",
      "Token IDs: torch.Size([25000, 768]), \n",
      "Masks: torch.Size([25000, 768])\n",
      "Class balance: 0=12500, 1=12500\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# 3. Creating attention masks and padding\n",
    "# =======================\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create attention masks (1 for real tokens, 0 for padding)\n",
    "def attention(ids):\n",
    "    return [1] * len(ids)\n",
    "\n",
    "df[\"attention_mask\"] = df[\"tokenized_text\"].parallel_apply(attention)\n",
    "\n",
    "# Pad sequences to max_length\n",
    "def padding(series, max_len=CONFIG[\"max_seq_len\"], pad_value=tokenizer.token_to_id(\"[PAD]\")):\n",
    "    tensors = []\n",
    "    for x in series:\n",
    "        truncated = x[:max_len]\n",
    "        padded = torch.nn.functional.pad(\n",
    "            torch.tensor(truncated),\n",
    "            (0, max_len - len(truncated)),  # Pad right side\n",
    "            value=pad_value\n",
    "        )\n",
    "        tensors.append(padded)\n",
    "    return torch.stack(tensors)\n",
    "    \n",
    "padded_ids = padding(df[\"tokenized_text\"])\n",
    "padded_masks = padding(df[\"attention_mask\"])\n",
    "labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "\n",
    "# Verify data shapes\n",
    "assert padded_ids.shape == (len(df), CONFIG[\"max_seq_len\"])\n",
    "assert padded_masks.shape == (len(df), CONFIG[\"max_seq_len\"])\n",
    "assert len(labels) == len(df)\n",
    "\n",
    "# =======================\n",
    "# 4. Save prepared data\n",
    "# =======================\n",
    "\n",
    "torch.save(padded_ids, CONFIG[\"padded_token_ids\"])\n",
    "torch.save(padded_masks, CONFIG[\"padded_attention_masks\"])\n",
    "torch.save(labels, CONFIG[\"sentiment_labels\"])\n",
    "\n",
    "print(f\"Data prepared - Samples: {len(labels)}, \\nShapes:\")\n",
    "print(f\"Token IDs: {padded_ids.shape}, \\nMasks: {padded_masks.shape}\")\n",
    "print(f\"Class balance: 0={sum(labels==0)}, 1={sum(labels==1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14da9e-e136-4e5e-9da0-74f3781cd332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_review_mlm",
   "language": "python",
   "name": "movie_review_mlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
