{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bbc789-1752-4b32-95de-d92772ff2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6169d0e11246ff8b891ef4a66f9d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=9375), Label(value='0 / 9375'))), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This script build tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "from tokenizers import Tokenizer\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "from functools import partial\n",
    "import emoji\n",
    "\n",
    "CONFIG = {\n",
    "    \"tokenizer_path\": \"movie_review_tokenizer.json\",\n",
    "    \"nmb_workers\": 8\n",
    "}\n",
    "\n",
    "# Load IMDb train and unsepervised files\n",
    "df1 = pd.read_parquet(\"data/IMDb/raw/train-00000-of-00001.parquet\")\n",
    "df2 = pd.read_parquet(\"data/IMDb/raw/unsupervised-00000-of-00001.parquet\")\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "def clean_text(text, bs4_parser, regex_module, emoji_module):\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove HTML tags\n",
    "    text = bs4_parser(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = regex_module.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=regex_module.MULTILINE)\n",
    "\n",
    "    # 4. Remove emojis\n",
    "    text = emoji_module.replace_emoji(text, replace='')\n",
    "    \n",
    "    # 5. Remove problematic chars (keep emoticons)\n",
    "    text = regex_module.sub(r'[\\x00-\\x1F\\x7F-\\x9F\\u2000-\\u200F\\u2028-\\u202F]', '', text)\n",
    "    text = regex_module.sub(\n",
    "        r'[^\\w\\s.,!?\\':)(/-=;]', \n",
    "        '', \n",
    "        str(text)\n",
    "    )\n",
    "    text = regex_module.sub(r'(?<!\\w)[:=/(](?!\\w)', '', text)\n",
    "    \n",
    "    # 6. Remove extra whitespace\n",
    "    text = regex_module.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create a pre-configured cleaning function and apply in parallel\n",
    "clean_text_optimized = partial(clean_text, bs4_parser=BeautifulSoup, regex_module=re, emoji_module=emoji)\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=CONFIG[\"nmb_workers\"])\n",
    "df[\"cleaned_text\"] = df[\"text\"].parallel_apply(clean_text_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176eb332-0f13-406a-ba1f-cd4a62ea3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is built\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, Regex\n",
    "\n",
    "# Initialize tokenizer (WordPiece/BPE)\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=30_000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    min_frequency=10,\n",
    ")\n",
    "\n",
    "# Splitting tokens on whitespace, puntuation and unknown chars\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Whitespace(),  # Split on whitespace\n",
    "    pre_tokenizers.Punctuation(),  # Split punctuation\n",
    "    pre_tokenizers.Split(\n",
    "        pattern=Regex(r\"[^\\w\\s]\"),  # Use tokenizers.Regex\n",
    "        behavior=\"isolated\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Convert DataFrame column to list for tqdm\n",
    "texts = df[\"cleaned_text\"].tolist()\n",
    "\n",
    "# Train on cleaned text\n",
    "tokenizer.train_from_iterator(df[\"cleaned_text\"], trainer=trainer)\n",
    "tokenizer.save(CONFIG[\"tokenizer_path\"])\n",
    "print(\"Tokenizer is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e73702-7c09-488e-93b4-ecb91d82d464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_review_mlm",
   "language": "python",
   "name": "movie_review_mlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
