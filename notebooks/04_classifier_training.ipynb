{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb1304-8028-4c14-9888-509ac3e16790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "from torch.nn.functional import scaled_dot_product_attention  # For flash attention\n",
    "\n",
    "# =======================\n",
    "# Load CONFIG\n",
    "# =======================\n",
    "CONFIG = {\n",
    "    \"d_model\": 512,\n",
    "    \"nhead\": 8,\n",
    "    \"num_layers\": 6,\n",
    "    \"dim_feedforward\": 2048,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 24,\n",
    "    \"max_seq_len\": 768,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"mixed_precision\": True,\n",
    "    \"flash_attention\": True,\n",
    "    \"checkpoint_interval\": 2,\n",
    "    \"checkpoint_dir\": \"model_FT_checkpoints\",\n",
    "    \"train_ratio\": 0.9,\n",
    "    \"tokenizer_path\": \"movie_review_tokenizer.json\",\n",
    "    \"token_ids_path\": \"padded_token_ids_FT.pt\",\n",
    "    \"attention_mask_path\": \"padded_attention_masks_FT.pt\",\n",
    "    \"sentiment_labels_path\": \"sentiment_labels.pt\",\n",
    "    \"mlm_checkpoint_path\": \"MLM_checkpoint_epoch_24.pt\"\n",
    "}\n",
    "\n",
    "# Ensure checkpoint directory exists\n",
    "import os\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# =======================\n",
    "# 1. Dataset Loading (Updated for your tokenizer)\n",
    "# =======================\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, token_ids, attention_mask, labels):\n",
    "        self.token_ids = token_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.token_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx].clone().detach().long(),\n",
    "        }\n",
    "\n",
    "# Load pre-tokenized data (update paths as needed)\n",
    "token_ids = torch.load(CONFIG[\"token_ids_path\"]).to(CONFIG[\"device\"])\n",
    "attention_mask = torch.load(CONFIG[\"attention_mask_path\"]).to(CONFIG[\"device\"])\n",
    "labels = torch.load(CONFIG[\"sentiment_labels_path\"])  \n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(len(labels) * CONFIG[\"train_ratio\"])\n",
    "train_dataset = SentimentDataset(token_ids[:train_size], attention_mask[:train_size], labels[:train_size])\n",
    "val_dataset = SentimentDataset(token_ids[train_size:], attention_mask[train_size:], labels[train_size:])\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"])\n",
    "\n",
    "# =======================\n",
    "# 2. Hybrid Head (Adjusted for d_model=512)\n",
    "# =======================\n",
    "class HybridClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size=CONFIG[\"d_model\"], num_classes=2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        mean_pool = token_embeddings.mean(dim=1)  # [B, H]\n",
    "        cnn_out = self.relu(self.cnn(token_embeddings.transpose(1, 2)))  # [B, H, T]\n",
    "        max_pool = self.pool(cnn_out).squeeze(-1)  # [B, H]\n",
    "        concat = torch.cat([mean_pool, max_pool], dim=-1)  # [B, 2*H]\n",
    "        return self.classifier(concat)\n",
    "\n",
    "# =======================\n",
    "# 3. Custom MLM Model (Updated for your architecture)\n",
    "# =======================\n",
    "class CustomMLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Load tokenizer if vocab_size not in config\n",
    "        if \"vocab_size\" not in config:\n",
    "            tokenizer = Tokenizer.from_file(config[\"tokenizer_path\"])\n",
    "            config[\"vocab_size\"] = tokenizer.get_vocab_size()\n",
    "            \n",
    "        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"])\n",
    "        self.pos_encoder = nn.Embedding(config[\"max_seq_len\"], config[\"d_model\"])\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config[\"d_model\"],\n",
    "            nhead=config[\"nhead\"],\n",
    "            dim_feedforward=config[\"dim_feedforward\"],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config[\"num_layers\"])\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        seq_len = input_ids.size(1)\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        embeddings = self.embedding(input_ids) + self.pos_encoder(positions)\n",
    "        \n",
    "        if CONFIG[\"flash_attention\"]:\n",
    "            embeddings = self.encoder(embeddings, mask=None, src_key_padding_mask=~attention_mask.bool())\n",
    "        else:\n",
    "            embeddings = self.encoder(embeddings, src_key_padding_mask=~attention_mask.bool())\n",
    "        return embeddings\n",
    "\n",
    "# Initialize MLM\n",
    "mlm_model = CustomMLM(CONFIG).to(CONFIG[\"device\"])\n",
    "\n",
    "# Load checkpoint properly\n",
    "checkpoint = torch.load(CONFIG[\"mlm_checkpoint_path\"], map_location=CONFIG[\"device\"])\n",
    "state_dict = checkpoint.get(\"model_state\", checkpoint)  # Handles both formats\n",
    "\n",
    "# Key renaming function\n",
    "def fix_key_names(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"transformer.layers\"):\n",
    "            new_key = k.replace(\"transformer.layers\", \"encoder.layers\")\n",
    "            new_state_dict[new_key] = v\n",
    "        elif k == \"pos_encoder.pe\":\n",
    "            new_state_dict[\"pos_encoder.weight\"] = v  # Positional encoding\n",
    "        elif k in [\"fc.weight\", \"fc.bias\"]:\n",
    "            continue  # Skip classifier weights if not needed\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "# Load with renamed keys\n",
    "mlm_model.load_state_dict(fix_key_names(state_dict), strict=False)\n",
    "\n",
    "# =======================\n",
    "# 4. Model Wrapper (Unfreeze last 2 layers)\n",
    "# =======================\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, mlm_model):\n",
    "        super().__init__()\n",
    "        self.mlm = mlm_model\n",
    "        self.head = HybridClassificationHead()\n",
    "\n",
    "        # Freeze all except last 2 layers and classifier\n",
    "        for param in self.mlm.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer in self.mlm.encoder.layers[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.mlm(input_ids, attention_mask)\n",
    "        return self.head(embeddings)\n",
    "\n",
    "model = SentimentClassifier(mlm_model).to(CONFIG[\"device\"])\n",
    "\n",
    "# =======================\n",
    "# 5. Training Setup (Optimizer, AMP, Gradient Clipping)\n",
    "# =======================\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=CONFIG[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.amp.GradScaler(enabled=CONFIG[\"mixed_precision\"])\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * CONFIG[\"epochs\"] // CONFIG[\"gradient_accumulation_steps\"],\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 6. Training Loop (Track Validation Accuracy)\n",
    "# =======================\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Train]\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "\n",
    "        with torch.amp.autocast(device_type=CONFIG[\"device\"], enabled=CONFIG[\"mixed_precision\"]):\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Val]\"):\n",
    "            input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "            attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "            labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1} — Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model and periodic checkpoints\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), f\"{CONFIG['checkpoint_dir']}/best_model.pt\")\n",
    "    if (epoch + 1) % CONFIG[\"checkpoint_interval\"] == 0:\n",
    "        torch.save(model.state_dict(), f\"{CONFIG['checkpoint_dir']}/epoch_{epoch+1}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f096c08-9780-420d-886e-8928bdbc3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "            attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "            labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Assuming you have a test dataset (modify as needed)\n",
    "test_dataset = SentimentDataset(\n",
    "    torch.load(\"padded_token_ids_test.pt\"),  # Replace with your test data paths\n",
    "    torch.load(\"padded_attention_masks_test.pt\"),\n",
    "    torch.load(\"sentiment_labels_test.pt\")\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"])\n",
    "\n",
    "# Load your trained model (assuming you have the best checkpoint)\n",
    "model_path = \"epoch_18.pt\"\n",
    "model = SentimentClassifier(mlm_model).to(CONFIG[\"device\"])\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(CONFIG[\"device\"])))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_model(model, test_loader)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14930e-c52f-4cb3-a23d-5a359dc1f7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_review_mlm",
   "language": "python",
   "name": "movie_review_mlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
